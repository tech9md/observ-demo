# OpenTelemetry Collector - Kubelet Stats for GKE Autopilot
# Deploys as DaemonSet to collect actual container/pod metrics from kubelet
# Based on: https://signoz.io/docs/opentelemetry-collection-agents/k8s/k8s-infra/kubelet-autogke/

# Image configuration
image:
  repository: otel/opentelemetry-collector-contrib

# CRITICAL: Deploy as DaemonSet (one per node)
mode: daemonset

# Service account for kubelet authentication
serviceAccount:
  create: true
  name: otel-collector-kubeletstats

# Cluster role for accessing kubelet metrics
clusterRole:
  create: true
  rules:
    - apiGroups: [""]
      resources: ["nodes/stats", "nodes/proxy"]
      verbs: ["get", "list", "watch"]

# CRITICAL: Use host network to access kubelet
hostNetwork: true

# GKE Autopilot minimum resources
resources:
  limits:
    cpu: 500m
    memory: 1Gi
  requests:
    cpu: 200m
    memory: 400Mi

# OpenTelemetry Collector Configuration
config:
  receivers:
    # Kubelet Stats receiver - collects container/pod metrics from kubelet
    kubeletstats:
      collection_interval: 20s
      auth_type: "serviceAccount"  # Use service account for GKE Autopilot
      endpoint: "https://${env:K8S_NODE_NAME}:10250"
      insecure_skip_verify: true
      metric_groups:
        - container
        - pod
        - node
      metrics:
        # CPU metrics
        k8s.node.cpu.utilization:
          enabled: true
        k8s.pod.cpu.utilization:
          enabled: true
        container.cpu.utilization:
          enabled: true
        # Memory metrics
        k8s.node.memory.working_set:
          enabled: true
        k8s.pod.memory.working_set:
          enabled: true
        container.memory.working_set:
          enabled: true
        # Network metrics
        k8s.pod.network.io:
          enabled: true
        k8s.node.network.io:
          enabled: true

  processors:
    batch:
      timeout: 10s
      send_batch_size: 512

    memory_limiter:
      check_interval: 1s
      limit_percentage: 75
      spike_limit_percentage: 25

    # Resource detection to add k8s metadata
    resourcedetection:
      detectors: [env, system]
      timeout: 5s

    # K8s attributes processor
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      extract:
        metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid

  exporters:
    # Prometheus exporter - exposes metrics for scraping
    prometheus:
      endpoint: "0.0.0.0:8889"
      namespace: "kubeletstats"
      send_timestamps: true
      metric_expiration: 5m
      resource_to_telemetry_conversion:
        enabled: true

    # Debug exporter for troubleshooting
    debug:
      verbosity: basic

  extensions:
    health_check:
      endpoint: 0.0.0.0:13133

  service:
    extensions: [health_check]
    pipelines:
      metrics:
        receivers: [kubeletstats]
        processors: [memory_limiter, resourcedetection, k8sattributes, batch]
        exporters: [prometheus, debug]

# Service configuration for Prometheus scraping
service:
  type: ClusterIP

# Pod annotations for Prometheus scraping
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8889"
  prometheus.io/path: "/metrics"

# Environment variables for node name
extraEnvs:
  - name: K8S_NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName
  - name: K8S_POD_NAME
    valueFrom:
      fieldRef:
        fieldPath: metadata.name
  - name: K8S_POD_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
  - name: K8S_POD_UID
    valueFrom:
      fieldRef:
        fieldPath: metadata.uid
  - name: K8S_POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP

# Ports to expose
ports:
  prometheus:
    enabled: true
    containerPort: 8889
    servicePort: 8889
    protocol: TCP
    hostPort: 8889  # Expose on host for node-level access
  health:
    enabled: true
    containerPort: 13133
    servicePort: 13133
    protocol: TCP

# Disable features not needed for kubelet stats
autoscaling:
  enabled: false

networkPolicy:
  enabled: false

# Tolerations to run on all nodes (including system nodes)
tolerations:
  - operator: "Exists"
