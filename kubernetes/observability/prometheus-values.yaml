# Prometheus Stack Configuration
# Includes: Prometheus, Alertmanager, Grafana, and exporters
# Using kube-prometheus-stack Helm chart
# Configured for GKE Autopilot compatibility
# OPTIMIZED FOR DEMO: Minimal resources for 5-10 concurrent users

# Global configuration
global:
  rbac:
    create: true

# Prometheus Operator
prometheusOperator:
  enabled: true
  # Deploy in observability namespace, not kube-system
  namespaces:
    releaseNamespace: true
    additional: []
  # Disable admission webhooks that need kube-system
  admissionWebhooks:
    enabled: false
  # Tolerate all taints to ensure scheduling
  tolerations:
    - operator: "Exists"
  # Reduced resources for better scheduling
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

# Prometheus Server
prometheus:
  enabled: true

  serviceAccount:
    create: true
    name: prometheus

  # Prometheus Server Configuration
  prometheusSpec:
    # Retention - reduced for demo (3 days is enough for testing)
    retention: 3d
    retentionSize: "3GB"

    # Tolerate all taints to ensure scheduling
    tolerations:
      - operator: "Exists"

    # Reduced resources for better scheduling
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 2Gi

    # Storage - reduced from 20Gi to 5Gi for demo
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

    # Service Monitor Selector - discover all service monitors
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}

    # Pod Monitor Selector
    podMonitorSelectorNilUsesHelmValues: false
    podMonitorSelector: {}

    # Scrape interval
    scrapeInterval: 30s
    evaluationInterval: 30s

    # Additional scrape configs for OpenTelemetry Collector
    additionalScrapeConfigs:
      - job_name: 'otel-collector'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - opentelemetry
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            action: keep
            regex: opentelemetry-collector
          - source_labels: [__meta_kubernetes_pod_container_port_name]
            action: keep
            regex: prometheus
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod

      # Microservices Demo metrics
      - job_name: 'microservices-demo'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - microservices-demo
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_label_app]
            target_label: app

      # Jaeger metrics
      - job_name: 'jaeger'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - observability
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: jaeger
          - source_labels: [__meta_kubernetes_pod_container_port_name]
            action: keep
            regex: metrics
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod

# Alertmanager - DISABLED for demo (saves ~1.5 CPU, 5Gi RAM)
# Alerts still fire in Prometheus, just no routing/silencing/grouping
alertmanager:
  enabled: false

  alertmanagerSpec:
    # GKE Autopilot minimum: 250m CPU, 512Mi memory
    resources:
      requests:
        cpu: 250m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi

    # CRITICAL: Reduce init container resources for GKE Autopilot
    # The prometheus-operator sets very high defaults that cause quota issues
    initContainers:
      - name: init-config-reloader
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi

    # CRITICAL: Reduce config-reloader sidecar resources
    containers:
      - name: config-reloader
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi

    # Storage - reduced from 2Gi to 1Gi for demo
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi

# Grafana - Disabled (we'll deploy separately for custom configuration)
grafana:
  enabled: false

# Node Exporter - DISABLED for GKE Autopilot
# (requires hostPID, hostNetwork, hostPath which are not allowed)
nodeExporter:
  enabled: false

# Prometheus Node Exporter subchart - DISABLED for GKE Autopilot
prometheus-node-exporter:
  enabled: false

# Kube State Metrics - Collect Kubernetes metrics
# Note: Using subchart configuration key 'kube-state-metrics'
kube-state-metrics:
  nodeSelector: {}
  affinity: {}
  # Tolerate all taints to ensure scheduling
  tolerations:
    - operator: "Exists"
  # Reduced resources for better scheduling
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

# Kubelet service - DISABLED for GKE Autopilot
# (cannot access kube-system resources)
kubelet:
  enabled: false

# Kube Controller Manager - DISABLED (not accessible in Autopilot)
kubeControllerManager:
  enabled: false

# Kube Scheduler - DISABLED (not accessible in Autopilot)
kubeScheduler:
  enabled: false

# Kube Proxy - DISABLED (not accessible in Autopilot)
kubeProxy:
  enabled: false

# Kube Etcd - DISABLED (not accessible in Autopilot)
kubeEtcd:
  enabled: false

# CoreDNS - DISABLED (cannot access kube-system)
coreDns:
  enabled: false

# Kube API Server - DISABLED (cannot access kube-system)
kubeApiServer:
  enabled: false

# Default Prometheus Rules - Adjusted for Autopilot
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: false  # Not accessible in Autopilot
    configReloaders: true
    general: true
    k8s: true
    kubeApiserverAvailability: false  # Not accessible in Autopilot
    kubeApiserverSlos: false  # Not accessible in Autopilot
    kubelet: false  # Not accessible in Autopilot
    kubeProxy: false  # Not accessible in Autopilot
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: false  # Needs node-exporter
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: false  # Not accessible in Autopilot
    kubeScheduler: false  # Not accessible in Autopilot
    kubeStateMetrics: true
    network: false  # Needs node-exporter
    node: false  # Needs node-exporter
    nodeExporterAlerting: false  # Needs node-exporter
    nodeExporterRecording: false  # Needs node-exporter
    prometheus: true
    prometheusOperator: true

# Additional Prometheus Rules for SRE practices
additionalPrometheusRulesMap:
  microservices-slo-rules:
    groups:
      - name: microservices_slos
        interval: 30s
        rules:
          # Request Rate (RPS)
          - record: microservices:request_rate:1m
            expr: sum(rate(http_server_requests_total[1m])) by (service, namespace)

          # Error Rate
          - record: microservices:error_rate:1m
            expr: sum(rate(http_server_requests_total{status=~"5.."}[1m])) by (service, namespace)

          # Error Ratio
          - record: microservices:error_ratio:1m
            expr: |
              sum(rate(http_server_requests_total{status=~"5.."}[1m])) by (service, namespace)
              /
              sum(rate(http_server_requests_total[1m])) by (service, namespace)

          # Latency P50
          - record: microservices:latency:p50
            expr: histogram_quantile(0.50, sum(rate(http_server_duration_bucket[1m])) by (service, le))

          # Latency P95
          - record: microservices:latency:p95
            expr: histogram_quantile(0.95, sum(rate(http_server_duration_bucket[1m])) by (service, le))

          # Latency P99
          - record: microservices:latency:p99
            expr: histogram_quantile(0.99, sum(rate(http_server_duration_bucket[1m])) by (service, le))

          # Availability (uptime)
          - record: microservices:availability:1h
            expr: |
              1 - (
                sum(rate(http_server_requests_total{status=~"5.."}[1h])) by (service)
                /
                sum(rate(http_server_requests_total[1h])) by (service)
              )

  microservices-alerts:
    groups:
      - name: microservices_slo_alerts
        interval: 1m
        rules:
          # High Error Rate Alert
          - alert: HighErrorRate
            expr: microservices:error_ratio:1m > 0.05
            for: 5m
            labels:
              severity: warning
              slo: error_rate
            annotations:
              summary: "High error rate detected for {{ $labels.service }}"
              description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

          # Critical Error Rate
          - alert: CriticalErrorRate
            expr: microservices:error_ratio:1m > 0.10
            for: 2m
            labels:
              severity: critical
              slo: error_rate
            annotations:
              summary: "CRITICAL: Very high error rate for {{ $labels.service }}"
              description: "Error rate is {{ $value | humanizePercentage }} (threshold: 10%)"

          # High Latency P95
          - alert: HighLatencyP95
            expr: microservices:latency:p95 > 1.0
            for: 5m
            labels:
              severity: warning
              slo: latency
            annotations:
              summary: "High P95 latency for {{ $labels.service }}"
              description: "P95 latency is {{ $value }}s (threshold: 1s)"

          # High Latency P99
          - alert: HighLatencyP99
            expr: microservices:latency:p99 > 2.0
            for: 5m
            labels:
              severity: warning
              slo: latency
            annotations:
              summary: "High P99 latency for {{ $labels.service }}"
              description: "P99 latency is {{ $value }}s (threshold: 2s)"

          # Low Availability
          - alert: LowAvailability
            expr: microservices:availability:1h < 0.99
            for: 5m
            labels:
              severity: critical
              slo: availability
            annotations:
              summary: "SLO violation: Low availability for {{ $labels.service }}"
              description: "Availability is {{ $value | humanizePercentage }} (SLO: 99%)"

          # Service Down
          - alert: ServiceDown
            expr: up{job="microservices-demo"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Service {{ $labels.pod }} is down"
              description: "The service has been down for more than 1 minute"
